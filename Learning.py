import torch
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_checker import check_env
from gymnasium.wrappers import TimeLimit
from Env_Rainforce_basic import EmotionBalanceEnv

# âœ… GPU í™•ì¸ ì¶œë ¥
if torch.cuda.is_available():
    print(f"âœ… GPU ì‚¬ìš© ì¤‘: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ â€” í˜„ì¬ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# âœ… ì§„í–‰ë¥  ì¶œë ¥ ì½œë°±
class PrintStepCallback(BaseCallback):
    def _on_step(self):
        if self.n_calls % 1000 == 0:
            print(f"ğŸ”„ Step {self.n_calls} ì§„í–‰ ì¤‘...")
        return True

# âœ… í•™ìŠµ or í”Œë ˆì´ ì„ íƒ
MODE = input("enter the mode (train or play) : ")  # "train" or "play"

if MODE == "train":
    # âœ… í•™ìŠµ í™˜ê²½ ì„¤ì • (ë Œë”ë§ ë¹„í™œì„±í™”)
    env = EmotionBalanceEnv(enabled=False)
    check_env(env)

    eval_env = Monitor(TimeLimit(EmotionBalanceEnv(enabled=False), max_episode_steps=300))
    eval_env.render = lambda *args, **kwargs: None

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/",
        eval_freq=10000,
        deterministic=True,
        render=False
    )


    # âœ… DQN ëª¨ë¸ ì •ì˜ (DoubleDQN + DuelingDQN)
    model = DQN(
        policy="MlpPolicy",
        env=env,
        learning_rate=1e-3,
        buffer_size=50000,
        learning_starts=1000,
        batch_size=128,
        gamma=0.99,
        train_freq=4,
        target_update_interval=500,
        exploration_initial_eps=1.0,
        exploration_final_eps=0.3,
        exploration_fraction=0.5,
        policy_kwargs=dict(net_arch=[128, 128, 64]),
        verbose=1,
        tensorboard_log="./dqn_tensorboard/",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # âœ… í•™ìŠµ ì‹¤í–‰
    model.learn(total_timesteps=1_000_000, callback=[eval_callback, PrintStepCallback()])

    # âœ… ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥
    model.save("dqn_emotion_balance")
    print("âœ… í•™ìŠµ ì™„ë£Œ. best_model.zip ë° dqn_emotion_balance.zip ì €ì¥ë¨.")

elif MODE == "play":
    model_path = "./best_model/best_model.zip"
    print(f"âœ… ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

    # âœ… í”Œë ˆì´ìš© í™˜ê²½: enabled=True (ë Œë”ë§), headless=False (ì°½ ìƒì„±)
    play_env = EmotionBalanceEnv(enabled=True)

    model = DQN.load(model_path)

    obs, _ = play_env.reset()
    done = False
    step_count = 0
    max_steps = 10000  # ì•ˆì „ì¥ì¹˜

    while not done and step_count < max_steps:
        action, _ = model.predict(obs, deterministic=True)
        obs, _, done, _, _ = play_env.step(action)
        play_env.render()
        step_count += 1

    print("âœ… í”Œë ˆì´ ì™„ë£Œ.")

