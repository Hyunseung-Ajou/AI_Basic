import torch
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_checker import check_env
from gymnasium.wrappers import TimeLimit
from Env_Rainforce_Active import EmotionBalanceEnv
import numpy as np

if torch.cuda.is_available():
    print(f"âœ… GPU ì‚¬ìš© ì¤‘: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ â€” í˜„ì¬ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

class PrintStepCallback(BaseCallback):
    def _on_step(self):
        if self.n_calls % 1000 == 0:
            print(f"ğŸ”„ Step {self.n_calls} ì§„í–‰ ì¤‘...")
        return True

MODE = input("Enter the mode (train or play) : ").lower()

if MODE == "train":
    # í™˜ê²½ ì´ˆê¸°í™”
    env = Monitor(TimeLimit(EmotionBalanceEnv(enabled=False), max_episode_steps=1000))
    eval_env = Monitor(TimeLimit(EmotionBalanceEnv(enabled=False), max_episode_steps=1000))

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/",
        eval_freq=10000,
        deterministic=True,
        render=False
    )

    model = DQN(
        policy="MlpPolicy",
        env=env,
        learning_rate=1e-3,
        buffer_size=50000,
        learning_starts=1000,
        batch_size=128,
        gamma=0.99,
        train_freq=4,
        target_update_interval=500,
        exploration_initial_eps=1.0,
        exploration_final_eps=0.3,
        exploration_fraction=0.5,
        policy_kwargs=dict(net_arch=[256, 256, 128]),
        verbose=1,
        tensorboard_log="./dqn_tensorboard/",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    model.learn(total_timesteps=1_000_000, callback=[eval_callback, PrintStepCallback()])
    model.save("dqn_emotion_balance")
    print("âœ… í•™ìŠµ ì™„ë£Œ. ëª¨ë¸ ì €ì¥ë¨ (dqn_emotion_balance.zip)")

elif MODE == "play":
    model_path = "./best_model/best_model.zip"
    model = DQN.load(model_path, device="cuda" if torch.cuda.is_available() else "cpu")

    n_episodes = 10
    success_count = 0
    rewards = []

    for episode in range(n_episodes):
        play_env = EmotionBalanceEnv(enabled=True)
        obs, _ = play_env.reset()
        done = False
        step_count = 0
        max_steps = 1000
        episode_reward = 0

        while not done and step_count < max_steps:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, _, info = play_env.step(action)
            play_env.render()
            episode_reward += reward
            step_count += 1

        if info.get("success", False):
            success_count += 1
        rewards.append(episode_reward)
        print(f"ğŸ® Episode {episode + 1}: Reward={episode_reward:.2f}, Success={info.get('success', False)}")

    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ì„±ê³µë¥  {success_count}/{n_episodes} ({(success_count / n_episodes) * 100:.1f}%)")
    print(f"í‰ê·  ë³´ìƒ: {np.mean(rewards):.2f}")

else:
    print("âš ï¸ MODEë¥¼ ì˜ëª» ì…ë ¥í•˜ì…¨ìŠµë‹ˆë‹¤. 'train' ë˜ëŠ” 'play'ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
